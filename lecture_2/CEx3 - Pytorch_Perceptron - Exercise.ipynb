{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dMecbQGx5F"
      },
      "source": [
        "# Perceptron with PyTorch: High-Level Neural Network Framework\n",
        "\n",
        "In this notebook, we'll implement a perceptron using PyTorch, showcasing the powerful abstractions that modern deep learning frameworks provide.\n",
        "\n",
        "## Comparison with NumPy Implementation\n",
        "\n",
        "**What PyTorch gives us:**\n",
        "- ✅ Automatic differentiation (autograd)\n",
        "- ✅ Built-in optimizers (SGD, Adam, etc.)\n",
        "- ✅ GPU acceleration with minimal code changes\n",
        "- ✅ Pre-built loss functions\n",
        "- ✅ Model serialization/loading\n",
        "- ✅ Seamless integration with complex architectures\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Understand PyTorch tensors and autograd\n",
        "2. Implement perceptrons using `nn.Module`\n",
        "3. Use built-in optimizers and loss functions\n",
        "4. Compare performance and simplicity with NumPy\n",
        "5. Scale to more complex problems effortlessly"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. GPU Setup"
      ],
      "metadata": {
        "id": "TgXRCNnbIJKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ],
      "metadata": {
        "id": "uEUnFxbjIHV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "-XnGVEHFIIqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8egQskEGx5G"
      },
      "source": [
        "## 1. PyTorch Setup and Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnkFRQyRGx5G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou6Iiw40Gx5G"
      },
      "source": [
        "## 2. PyTorch Tensors vs NumPy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awf5XYolGx5G"
      },
      "outputs": [],
      "source": [
        "# Compare NumPy arrays and PyTorch tensors\n",
        "print(\"NumPy vs PyTorch comparison:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# NumPy array\n",
        "np_array = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
        "print(f\"NumPy array:\\n{np_array}\")\n",
        "print(f\"Type: {type(np_array)}\")\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "torch_tensor = torch.from_numpy(np_array)\n",
        "print(f\"\\nPyTorch tensor:\\n{torch_tensor}\")\n",
        "print(f\"Type: {type(torch_tensor)}\")\n",
        "print(f\"Device: {torch_tensor.device}\")\n",
        "print(f\"Requires grad: {torch_tensor.requires_grad}\")\n",
        "\n",
        "# Enable gradient computation\n",
        "torch_tensor.requires_grad_(True)\n",
        "print(f\"After enabling grad: {torch_tensor.requires_grad}\")\n",
        "\n",
        "# Demonstrate automatic differentiation\n",
        "print(\"\\nAutomatic differentiation demo:\")\n",
        "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
        "y = x ** 2\n",
        "z = y.sum()\n",
        "\n",
        "print(f\"x = {x}\")\n",
        "print(f\"y = x² = {y}\")\n",
        "print(f\"z = sum(y) = {z}\")\n",
        "\n",
        "# Backpropagation\n",
        "z.backward()\n",
        "print(f\"dz/dx = {x.grad}\")\n",
        "print(\"Expected: [2*1, 2*2] = [2, 4] ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puq1qPN2Gx5G"
      },
      "source": [
        "## 3. Simple Perceptron Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnBN0k3wGx5G"
      },
      "outputs": [],
      "source": [
        "class SimplePerceptron(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimplePerceptron, self).__init__()\n",
        "        # Single linear layer (this includes both weights and bias!)\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through linear layer\n",
        "        return self.linear(x)\n",
        "\n",
        "# Create perceptron\n",
        "perceptron = SimplePerceptron(input_size=2)\n",
        "print(\"Simple Perceptron created!\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in perceptron.parameters())}\")\n",
        "\n",
        "# Inspect the parameters\n",
        "for name, param in perceptron.named_parameters():\n",
        "    print(f\"{name}: shape {param.shape}, values {param.data}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.tensor([[1.0, 2.0]])\n",
        "output = perceptron(test_input)\n",
        "print(f\"\\nTest input: {test_input}\")\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.B. Visualizing our network"
      ],
      "metadata": {
        "id": "gqpy2mwS2j9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.B.1. Visualizing with torchviz"
      ],
      "metadata": {
        "id": "PPQOvNih26AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchviz"
      ],
      "metadata": {
        "id": "kQyl09iW2wbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot, make_dot_from_trace\n",
        "make_dot(perceptron(test_input), params=dict(perceptron.named_parameters()))"
      ],
      "metadata": {
        "id": "KBsbmOoyxqOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.B.2. Visualizing with netron"
      ],
      "metadata": {
        "id": "OooC66j53D8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The repo is here: https://github.com/lutzroeder/netron, containing downloadable apps, but we can also use the website.\n",
        "\n",
        "1. Run this code:"
      ],
      "metadata": {
        "id": "8fZwULew3KmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_names = ['X']\n",
        "output_names = ['y_hat']\n",
        "torch.onnx.export(perceptron, test_input, 'perceptron.onnx', input_names=input_names, output_names=output_names)"
      ],
      "metadata": {
        "id": "3pCa41dM03QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Click on the folder icon to the left and download `perceptron.onnx`.\n",
        "\n",
        "3. Go to https://netron.app/ and upload the file there.\n",
        "\n",
        "(`Gemm` = \"General Matrix multiplication\")"
      ],
      "metadata": {
        "id": "0t55PkIe3WQh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCr-XXYhGx5H"
      },
      "source": [
        "## 4. Training on Logic Gates with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwQrzxj1Gx5H"
      },
      "outputs": [],
      "source": [
        "# Prepare AND gate data\n",
        "X_and = torch.tensor([[0.0, 0.0],\n",
        "                      [0.0, 1.0],\n",
        "                      [1.0, 0.0],\n",
        "                      [1.0, 1.0]])\n",
        "\n",
        "y_and = torch.tensor([[0.0], [0.0], [0.0], [1.0]])\n",
        "\n",
        "print(\"AND Gate Data:\")\n",
        "print(f\"Inputs:\\n{X_and}\")\n",
        "print(f\"Labels:\\n{y_and}\")\n",
        "\n",
        "# Create model, loss function, and optimizer\n",
        "model = SimplePerceptron(input_size=2)\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy with logits\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "print(f\"\\nModel: {model}\")\n",
        "print(f\"Loss function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer}\")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "print(\"\\nTraining...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_and)\n",
        "    loss = criterion(outputs, y_and)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "    loss.backward()        # Compute gradients\n",
        "    optimizer.step()       # Update weights\n",
        "\n",
        "    # Record loss\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Test the trained model\n",
        "print(\"\\nTesting trained model:\")\n",
        "with torch.no_grad():  # No need to compute gradients for testing\n",
        "    test_outputs = model(X_and)\n",
        "    predictions = torch.sigmoid(test_outputs) > 0.5\n",
        "\n",
        "    for i in range(len(X_and)):\n",
        "        print(f\"Input: {X_and[i].numpy()}, Expected: {y_and[i].item():.0f}, \"\n",
        "              f\"Predicted: {predictions[i].item():.0f}, \"\n",
        "              f\"Probability: {torch.sigmoid(test_outputs[i]).item():.3f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses, linewidth=2)\n",
        "plt.title('Training Loss - AND Gate (PyTorch)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsTLHFr-Gx5H"
      },
      "source": [
        "## 6. Multiple Logic Gates with Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1: Train and test the SimplePerceptron on the OR and XOR problems"
      ],
      "metadata": {
        "id": "GxV3Y3xTIBoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUcJ6AgtGx5I"
      },
      "source": [
        "## 7. More Complex Dataset with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a larger, more complex dataset\n",
        "X, y = make_classification(n_samples=1000,\n",
        "                          n_features=10,\n",
        "                          n_informative=5,\n",
        "                          n_redundant=2,\n",
        "                          n_clusters_per_class=1,\n",
        "                          random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "print(f\"Training data: {X_train_tensor.shape}\")\n",
        "print(f\"Test data: {X_test_tensor.shape}\")\n",
        "print(f\"Features: {X.shape[1]}\")\n",
        "print(f\"Classes: {len(np.unique(y))}\")"
      ],
      "metadata": {
        "id": "WI7VXAPjIR3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: Train the SimplePerceptron on this complex dataset,\n",
        "# and test it on the test set"
      ],
      "metadata": {
        "id": "sRilbIwVIXBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAFWreMXGx5I"
      },
      "source": [
        "## 8. Model Inspection and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7nlpIe1Gx5I"
      },
      "outputs": [],
      "source": [
        "# Inspect the trained model\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\nModel Parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}\")\n",
        "    if param.dim() == 1:  # bias\n",
        "        print(f\"  Value: {param.data.item():.4f}\")\n",
        "    else:  # weights\n",
        "        print(f\"  Min: {param.data.min():.4f}, Max: {param.data.max():.4f}\")\n",
        "        print(f\"  Mean: {param.data.mean():.4f}, Std: {param.data.std():.4f}\")\n",
        "\n",
        "# Feature importance (absolute weights)\n",
        "weights = model.linear.weight.data.abs().squeeze()\n",
        "feature_importance = weights / weights.sum()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(feature_importance)), feature_importance.numpy())\n",
        "plt.title('Feature Importance (Normalized Absolute Weights)')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Relative Importance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Model complexity comparison\n",
        "print(\"\\nModel Complexity:\")\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Model size: {total_params * 4 / 1024:.2f} KB (float32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3u1ZmE0Gx5I"
      },
      "source": [
        "## 9. Model Saving and Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model_path = 'perceptron_model.pth'\n",
        "\n",
        "# Method 1: Save state dict (recommended)\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Create a new model instance\n",
        "loaded_model = RobustPerceptron(input_size=X.shape[1])\n",
        "\n",
        "# Load the saved state dict\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "VUFa8WAaIjoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umUAlr6ZGx5I"
      },
      "outputs": [],
      "source": [
        "# Q3 : Verify that loaded model produces same results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Save entire model (for educational purposes)\n",
        "torch.save(model, 'complete_model.pth')\n",
        "complete_loaded_model = torch.load('complete_model.pth', weights_only=False)\n",
        "print(\"\\nComplete model saved and loaded as well!\")\n",
        "\n",
        "# Clean up files\n",
        "import os\n",
        "os.remove(model_path)\n",
        "os.remove('complete_model.pth')\n",
        "print(\"Temporary files cleaned up.\")"
      ],
      "metadata": {
        "id": "PhPuUj4RIlik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS95gGa6Gx5I"
      },
      "source": [
        "## 10. GPU Acceleration Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6RVW6C7Gx5I"
      },
      "outputs": [],
      "source": [
        "# Demonstrate GPU vs CPU performance (if GPU is available)\n",
        "import time\n",
        "\n",
        "# Create larger dataset for timing\n",
        "large_X = torch.randn(10000, 100)\n",
        "large_y = torch.randint(0, 2, (10000, 1)).float()\n",
        "\n",
        "# CPU timing\n",
        "cpu_model = SimplePerceptron(100)\n",
        "cpu_model = cpu_model.to('cpu')\n",
        "cpu_X = large_X.to('cpu')\n",
        "cpu_y = large_y.to('cpu')\n",
        "\n",
        "NPASSES = 2000\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(NPASSES):\n",
        "    output = cpu_model(cpu_X)\n",
        "    loss = F.binary_cross_entropy_with_logits(output, cpu_y)\n",
        "    loss.backward()\n",
        "cpu_time = time.time() - start_time\n",
        "\n",
        "print(f\"CPU time for {NPASSES} forward+backward passes: {cpu_time:.4f}s\")\n",
        "\n",
        "# GPU timing (if available)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_model = SimplePerceptron(100)\n",
        "    gpu_model = gpu_model.to('cuda')\n",
        "    gpu_X = large_X.to('cuda')\n",
        "    gpu_y = large_y.to('cuda')\n",
        "\n",
        "    # Warm up GPU\n",
        "    for _ in range(5):\n",
        "        _ = gpu_model(gpu_X)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(NPASSES):\n",
        "        output = gpu_model(gpu_X)\n",
        "        loss = F.binary_cross_entropy_with_logits(output, gpu_y)\n",
        "        loss.backward()\n",
        "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
        "    gpu_time = time.time() - start_time\n",
        "\n",
        "    print(f\"GPU time for {NPASSES} forward+backward passes: {gpu_time:.4f}s\")\n",
        "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
        "else:\n",
        "    print(\"GPU not available for speed comparison\")\n",
        "\n",
        "# Memory usage\n",
        "print(f\"\\nMemory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\" if torch.cuda.is_available() else \"\\nCPU mode - no GPU memory tracking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f519fb3Gx5I"
      },
      "source": [
        "## 11. Comparison: NumPy vs PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lOgOEWoGx5I"
      },
      "outputs": [],
      "source": [
        "# Side-by-side comparison of NumPy and PyTorch implementations\n",
        "print(\"NumPy vs PyTorch Comparison\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "comparison_table = \"\"\"\n",
        "| Aspect                | NumPy Implementation           | PyTorch Implementation         |\n",
        "|----------------------|--------------------------------|--------------------------------|\n",
        "| **Gradient Computation** | Manual derivative calculation  | Automatic differentiation      |\n",
        "| **Optimization**     | Manual weight updates          | Built-in optimizers (SGD, Adam)|\n",
        "| **Loss Functions**   | Manual implementation          | Pre-built functions            |\n",
        "| **GPU Support**      | Not available                  | Seamless with .to('cuda')     |\n",
        "| **Activation Functions** | Manual implementation      | Built-in functions             |\n",
        "| **Model Saving**     | Custom serialization           | Built-in save/load             |\n",
        "| **Scalability**      | Limited                        | Easy to extend to deep networks|\n",
        "| **Learning Curve**   | Educational, shows details     | Production-ready              |\n",
        "| **Lines of Code**    | ~100+ lines                    | ~20 lines                     |\n",
        "\"\"\"\n",
        "\n",
        "print(comparison_table)\n",
        "\n",
        "# Code comparison\n",
        "print(\"\\nCode Comparison - Training Loop:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"NumPy (manual):\")\n",
        "numpy_code = \"\"\"\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z = np.dot(X, weights) + bias\n",
        "    predictions = sigmoid(z)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = -np.mean(y * np.log(predictions) + (1-y) * np.log(1-predictions))\n",
        "\n",
        "    # Backward pass (manual gradients)\n",
        "    dz = predictions - y\n",
        "    dw = np.dot(X.T, dz) / len(X)\n",
        "    db = np.mean(dz)\n",
        "\n",
        "    # Update weights\n",
        "    weights -= learning_rate * dw\n",
        "    bias -= learning_rate * db\n",
        "\"\"\"\n",
        "print(numpy_code)\n",
        "\n",
        "print(\"PyTorch (automatic):\")\n",
        "pytorch_code = \"\"\"\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backward pass (automatic gradients)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\"\"\"\n",
        "print(pytorch_code)\n",
        "\n",
        "print(\"\\nKey Advantages of PyTorch:\")\n",
        "advantages = [\n",
        "    \"✅ Automatic differentiation - no manual gradient calculation\",\n",
        "    \"✅ Built-in optimizers with advanced features (momentum, learning rate scheduling)\",\n",
        "    \"✅ GPU acceleration with minimal code changes\",\n",
        "    \"✅ Extensive library of layers and functions\",\n",
        "    \"✅ Easy to scale from simple perceptron to complex deep networks\",\n",
        "    \"✅ Rich ecosystem and community support\",\n",
        "    \"✅ Production-ready with deployment tools\"\n",
        "]\n",
        "\n",
        "for advantage in advantages:\n",
        "    print(f\"  {advantage}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYE-1ePQGx5J"
      },
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We've Learned:\n",
        "\n",
        "1. **PyTorch Fundamentals**:\n",
        "   - Tensors and automatic differentiation\n",
        "   - Building models with `nn.Module`\n",
        "   - Training loops with optimizers and loss functions\n",
        "\n",
        "2. **Perceptron Implementation**:\n",
        "   - Simple perceptron in just a few lines\n",
        "   - Multiple activation functions\n",
        "   - Batch training and validation\n",
        "\n",
        "3. **Advanced Features**:\n",
        "   - GPU acceleration\n",
        "   - Model saving/loading\n",
        "   - Feature importance analysis\n",
        "\n",
        "### Key Advantages of PyTorch:\n",
        "\n",
        "- **Simplicity**: Complex operations in just a few lines\n",
        "- **Flexibility**: Easy to experiment and modify\n",
        "- **Performance**: GPU acceleration and optimized operations\n",
        "- **Scalability**: Same code works for simple perceptrons and complex networks\n",
        "- **Community**: Extensive documentation and examples\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Deep Learning Fundamentals**:\n",
        "   - Multi-layer perceptrons\n",
        "   - Backpropagation in detail\n",
        "   - Regularization techniques\n",
        "\n",
        "2. **Advanced Architectures**:\n",
        "   - Convolutional Neural Networks (CNNs)\n",
        "   - Recurrent Neural Networks (RNNs)\n",
        "   - Transformers\n",
        "\n",
        "3. **Practical Applications**:\n",
        "   - Computer vision\n",
        "   - Natural language processing\n",
        "   - Time series analysis\n",
        "\n",
        "### Resources for Further Learning:\n",
        "\n",
        "- [PyTorch Official Tutorials](https://pytorch.org/tutorials/)\n",
        "- [Deep Learning with PyTorch](https://pytorch.org/deep-learning-with-pytorch)\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
        "\n",
        "**Congratulations on completing your journey from NumPy to PyTorch! 🚀**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLHB-HLwI6wk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}