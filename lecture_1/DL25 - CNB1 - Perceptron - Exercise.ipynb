{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Implementing a Perceptron with NumPy\n",
    "\n",
    "In this notebook, we'll implement a perceptron from scratch using NumPy. By the end of this session, you'll understand:\n",
    "\n",
    "1. How to implement forward propagation\n",
    "2. Different activation functions\n",
    "3. Training a perceptron with simple rules\n",
    "4. Visualizing decision boundaries\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement a perceptron class with NumPy\n",
    "- Understand the role of weights, bias, and activation functions\n",
    "- Train a perceptron to solve simple classification problems\n",
    "- Visualize how the perceptron learns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "First, let's implement the activation functions we learned about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    \"\"\"\n",
    "    Step function: returns 1 if x >= 0, 0 otherwise\n",
    "    \"\"\"\n",
    "    # ========= EXERCISE 1 =========\n",
    "    # complete the function using a numpy function\n",
    "    # look at numpy's math functions:\n",
    "    # https://numpy.org/doc/2.2/reference/routines.math.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function: returns 1 / (1 + exp(-x))\n",
    "    \"\"\"\n",
    "    # Clip x to prevent overflow\n",
    "    x = np.clip(x, -250, 250)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent function\n",
    "    \"\"\"\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU function: returns x if x > 0\n",
    "    \"\"\"\n",
    "    # ========= EXERCISE 2 =========\n",
    "    # complete the function using a numpy function\n",
    "    # look at numpy's math functions:\n",
    "    # https://numpy.org/doc/2.2/reference/routines.math.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize these activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "functions = [step_function, sigmoid, tanh, relu]\n",
    "names = ['Step Function', 'Sigmoid', 'Tanh', 'ReLU']\n",
    "\n",
    "for i, (func, name) in enumerate(zip(functions, names)):\n",
    "    axes[i].plot(x, func(x), linewidth=2)\n",
    "    axes[i].set_title(f'{name}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_xlabel('Input (x)')\n",
    "    axes[i].set_ylabel('Output f(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the Perceptron Class\n",
    "\n",
    "Now let's implement our perceptron class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, activation='step', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the perceptron\n",
    "        \n",
    "        Parameters:\n",
    "        - n_inputs: number of input features\n",
    "        - activation: activation function ('step', 'sigmoid', 'tanh', 'relu')\n",
    "        - learning_rate: learning rate for training\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.normal(0, 0.1, n_inputs)\n",
    "        self.bias = np.random.normal(0, 0.1)\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activation_functions = {\n",
    "            'step': step_function,\n",
    "            'sigmoid': sigmoid,\n",
    "            'tanh': tanh,\n",
    "            'relu': relu\n",
    "        }\n",
    "        self.activation = self.activation_functions[activation]\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Keep track of training history\n",
    "        self.training_history = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input data (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - output after activation function\n",
    "        \"\"\"\n",
    "        # Calculate weighted sum: z = X @ weights + bias\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply activation function\n",
    "        activated_output = self.activation(linear_output)\n",
    "        \n",
    "        return activated_output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform one training step (for step function only)\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input data (n_samples, n_features)\n",
    "        - y: true labels (n_samples,)\n",
    "        \n",
    "        Returns:\n",
    "        - error: classification error\n",
    "        \"\"\"\n",
    "        # Make predictions\n",
    "        predictions = self.forward(X)\n",
    "        \n",
    "        # ========= EXERCISE 3 =========\n",
    "        # Calculate errors\n",
    "        errors = None # FIX ME!\n",
    "\n",
    "        # ========= EXERCISE 4 =========\n",
    "        # Update weights and bias using perceptron learning rule\n",
    "        # w = w + learning_rate * (y - y_pred) * x\n",
    "        self.weights += None # FIX ME!\n",
    "        # bias = learning_rate * sum(errors)\n",
    "        self.bias += None # FIX ME!\n",
    "        \n",
    "        # Calculate total error\n",
    "        total_error = np.sum(np.abs(errors))\n",
    "        \n",
    "        return total_error\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"\n",
    "        Train the perceptron\n",
    "        \n",
    "        Parameters:\n",
    "        - X: input data (n_samples, n_features)\n",
    "        - y: true labels (n_samples,)\n",
    "        - epochs: number of training epochs\n",
    "        \"\"\"\n",
    "        self.training_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # ========= EXERCISE 5 =========\n",
    "            # Perform one training step, by calling the train_step method correctly\n",
    "            # HINT: Recall the `self` Python keyword\n",
    "            error = None # FIX ME!\n",
    "            \n",
    "            # Record training history\n",
    "            self.training_history.append(error)\n",
    "            \n",
    "            # Print progress every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Error: {error}\")\n",
    "            \n",
    "            # Stop if converged (error is 0)\n",
    "            if error == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs!\")\n",
    "                break\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training history\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_history, linewidth=2)\n",
    "        plt.title('Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Total Error')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "print(\"Perceptron class implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test with Simple Data\n",
    "\n",
    "Let's test our perceptron with simple linearly separable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple linearly separable data\n",
    "# AND gate data\n",
    "X_and = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "\n",
    "y_and = np.array([0, 0, 0, 1])  # AND gate outputs\n",
    "\n",
    "print(\"AND Gate Data:\")\n",
    "print(\"Inputs:\")\n",
    "print(X_and)\n",
    "print(\"Labels:\")\n",
    "print(y_and)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_and == i\n",
    "    plt.scatter(X_and[mask, 0], X_and[mask, 1], \n",
    "               c=colors[i], label=f'Class {i}', s=100, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('AND Gate Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Perceptron on AND Gate\n",
    "\n",
    "Now let's train our perceptron to learn the AND gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train perceptron\n",
    "perceptron_and = Perceptron(n_inputs=2, activation='step', learning_rate=0.1)\n",
    "\n",
    "print(\"Initial weights:\", perceptron_and.weights)\n",
    "print(\"Initial bias:\", perceptron_and.bias)\n",
    "print(\"\\nTraining perceptron on AND gate...\")\n",
    "\n",
    "# Train the perceptron\n",
    "perceptron_and.train(X_and, y_and, epochs=20)\n",
    "\n",
    "print(\"\\nFinal weights:\", perceptron_and.weights)\n",
    "print(\"Final bias:\", perceptron_and.bias)\n",
    "\n",
    "# Test the trained perceptron\n",
    "print(\"\\nTesting trained perceptron:\")\n",
    "for i in range(len(X_and)):\n",
    "    prediction = perceptron_and.predict(X_and[i:i+1])[0]\n",
    "    print(f\"Input: {X_and[i]}, Expected: {y_and[i]}, Predicted: {prediction}\")\n",
    "\n",
    "# Plot training history\n",
    "perceptron_and.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Decision Boundary\n",
    "\n",
    "Let's visualize how the perceptron creates a decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(perceptron, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of the perceptron\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh of points\n",
    "    h = 0.01  # step size\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on the mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = perceptron.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot the data points\n",
    "    colors = ['red', 'blue']\n",
    "    for i in range(2):\n",
    "        mask = y == i\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], \n",
    "                   c=colors[i], label=f'Class {i}', s=100, alpha=0.8, edgecolors='black')\n",
    "    \n",
    "    # Plot the decision line (for 2D data)\n",
    "    if X.shape[1] == 2:\n",
    "        w1, w2 = perceptron.weights\n",
    "        b = perceptron.bias\n",
    "        \n",
    "        # Decision line: w1*x1 + w2*x2 + b = 0\n",
    "        # Solve for x2: x2 = -(w1*x1 + b) / w2\n",
    "        if w2 != 0:\n",
    "            x_line = np.linspace(x_min, x_max, 100)\n",
    "            y_line = -(w1 * x_line + b) / w2\n",
    "            plt.plot(x_line, y_line, 'k--', linewidth=2, label='Decision Boundary')\n",
    "    \n",
    "    plt.xlabel('Input 1')\n",
    "    plt.ylabel('Input 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for AND gate\n",
    "plot_decision_boundary(perceptron_and, X_and, y_and, \"AND Gate - Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try Different Logic Gates\n",
    "\n",
    "Let's test our perceptron on different logic gates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different logic gates\n",
    "gates = {\n",
    "    'OR': np.array([0, 1, 1, 1]),\n",
    "    'NAND': np.array([1, 1, 1, 0]),\n",
    "    'NOR': np.array([1, 0, 0, 0])\n",
    "}\n",
    "\n",
    "# Same input data for all gates\n",
    "X_gates = np.array([[0, 0],\n",
    "                    [0, 1], \n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "\n",
    "# Train perceptron on each gate\n",
    "perceptrons = {}\n",
    "\n",
    "for gate_name, y_gate in gates.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training perceptron on {gate_name} gate\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create and train perceptron\n",
    "    perceptron = Perceptron(n_inputs=2, activation='step', learning_rate=0.1)\n",
    "    perceptron.train(X_gates, y_gate, epochs=20)\n",
    "    \n",
    "    # Store the trained perceptron\n",
    "    perceptrons[gate_name] = perceptron\n",
    "    \n",
    "    # Test predictions\n",
    "    print(f\"\\nTesting {gate_name} gate:\")\n",
    "    for i in range(len(X_gates)):\n",
    "        prediction = perceptron.predict(X_gates[i:i+1])[0]\n",
    "        print(f\"Input: {X_gates[i]}, Expected: {y_gate[i]}, Predicted: {prediction}\")\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plot_decision_boundary(perceptron, X_gates, y_gate, f\"{gate_name} Gate - Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test with XOR Gate (Non-linearly Separable)\n",
    "\n",
    "Let's see what happens when we try the XOR gate, which is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR gate - not linearly separable!\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "print(\"XOR Gate Data:\")\n",
    "print(\"Inputs:\")\n",
    "print(X_gates)\n",
    "print(\"Labels:\")\n",
    "print(y_xor)\n",
    "\n",
    "# Visualize XOR data\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_xor == i\n",
    "    plt.scatter(X_gates[mask, 0], X_gates[mask, 1], \n",
    "               c=colors[i], label=f'Class {i}', s=100, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('XOR Gate Data (Not Linearly Separable)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Try to train perceptron on XOR\n",
    "print(\"\\nAttempting to train perceptron on XOR gate...\")\n",
    "perceptron_xor = Perceptron(n_inputs=2, activation='step', learning_rate=0.1)\n",
    "perceptron_xor.train(X_gates, y_xor, epochs=100)\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nTesting XOR gate:\")\n",
    "for i in range(len(X_gates)):\n",
    "    prediction = perceptron_xor.predict(X_gates[i:i+1])[0]\n",
    "    print(f\"Input: {X_gates[i]}, Expected: {y_xor[i]}, Predicted: {prediction}\")\n",
    "\n",
    "# Plot training history\n",
    "perceptron_xor.plot_training_history()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(perceptron_xor, X_gates, y_xor, \"XOR Gate - Decision Boundary (Failed)\")\n",
    "\n",
    "print(\"\\nNote: The perceptron cannot learn XOR because it's not linearly separable!\")\n",
    "print(\"This limitation led to the development of multi-layer perceptrons (neural networks).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different activation functions on AND gate\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, activation in enumerate(activations):\n",
    "    print(f\"\\nTesting {activation} activation:\")\n",
    "    \n",
    "    # Create perceptron with different activation\n",
    "    perceptron = Perceptron(n_inputs=2, activation=activation, learning_rate=0.1)\n",
    "    \n",
    "    # For non-step functions, we'll just show the outputs without training\n",
    "    # (since our training rule is designed for step function)\n",
    "    \n",
    "    # Test with random weights\n",
    "    outputs = perceptron.predict(X_and)\n",
    "    print(f\"Outputs with {activation}: {outputs}\")\n",
    "    \n",
    "    # Plot activation function\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = perceptron.activation(x)\n",
    "    \n",
    "    axes[i].plot(x, y, linewidth=2)\n",
    "    axes[i].set_title(f'{activation.title()} Activation')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_xlabel('Input')\n",
    "    axes[i].set_ylabel('Output')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Our training algorithm is specifically designed for the step function.\")\n",
    "print(\"For other activation functions, we would need gradient descent and backpropagation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Example: Iris Dataset (2D)\n",
    "\n",
    "Let's apply our perceptron to a real-world dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D dataset for binary classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate linearly separable data\n",
    "X_real, y_real = make_classification(n_samples=100, \n",
    "                                    n_features=2, \n",
    "                                    n_redundant=0, \n",
    "                                    n_informative=2,\n",
    "                                    random_state=42, \n",
    "                                    n_clusters_per_class=1,\n",
    "                                    class_sep=2.0)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'blue']\n",
    "for i in range(2):\n",
    "    mask = y_real == i\n",
    "    plt.scatter(X_real[mask, 0], X_real[mask, 1], \n",
    "               c=colors[i], label=f'Class {i}', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated 2D Dataset for Binary Classification')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= EXERCISE 6 =========\n",
    "# Train perceptron on this data\n",
    "print(\"Training perceptron on 2D dataset...\")\n",
    "perceptron_real = Perceptron(n_inputs=2, activation='step', learning_rate=0.01)\n",
    "# MISSING: train the perceptron instance on X_real and y_real,\n",
    "# and also choose the number of epochs to run for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= EXERCISE 7 =========\n",
    "# Calculate accuracy\n",
    "# Get the predictions out of your trianed model\n",
    "predictions = None # FIX ME!\n",
    "accuracy = np.mean(predictions == y_real)\n",
    "print(f\"\\nAccuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "plot_decision_boundary(perceptron_real, X_real, y_real, \"Real Dataset - Decision Boundary\")\n",
    "\n",
    "# Plot training history\n",
    "perceptron_real.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned:\n",
    "\n",
    "1. **Implemented a perceptron from scratch** using NumPy\n",
    "2. **Understanding of activation functions** and their roles\n",
    "3. **Forward propagation** and how perceptrons make predictions\n",
    "4. **Training using the perceptron learning rule**\n",
    "5. **Visualizing decision boundaries** and understanding limitations\n",
    "6. **Testing on different problems** including logic gates\n",
    "7. **Understanding the limitation** with non-linearly separable data (XOR)\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Perceptrons can only solve linearly separable problems**\n",
    "- **The choice of activation function affects the output range**\n",
    "- **Learning rate affects convergence speed and stability**\n",
    "- **Visualization helps understand what the model is learning**\n",
    "- **Real-world applications require careful data preprocessing**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Learn about **multi-layer perceptrons** (neural networks)\n",
    "2. Understand **backpropagation** for training deep networks\n",
    "3. Explore **different optimization algorithms** (SGD, Adam, etc.)\n",
    "4. Study **regularization techniques** to prevent overfitting\n",
    "5. Practice with **real datasets** and different problem types\n",
    "\n",
    "Great job completing this hands-on implementation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
